import json
from flask import Flask, request, jsonify
import nemo.collections.asr as nemo_asr
import torch
import gc
import os
import sys
import time
import logging
import subprocess
import difflib
import soundfile as sf
import librosa
import numpy as np
import pandas as pd, json
from pathlib import Path 
from segment import (
    segment_audio,
    frame_segment_audio,
    SegmentDataset
)
from torch.utils.data import DataLoader

from nemo.collections.asr.parts.preprocessing.segment import AudioSegment
from nemo.collections.asr.models import EncDecHybridRNNTCTCModel

from langchain_community.vectorstores import Qdrant
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA

from nemo.collections.asr.parts.utils.vad_utils import (
    prepare_manifest,
)
from nemo.collections.asr.parts.utils.manifest_utils import (
    create_manifest
)


app = Flask(__name__)

# –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–æ 2 –ì–ë
app.config['MAX_CONTENT_LENGTH'] = 2 * 1024 * 1024 * 1024  # 2 –ì–ë

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (CPU –∏–ª–∏ MPS –¥–ª—è Mac M1/M2)
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("–ò—Å–ø–æ–ª—å–∑—É—é MPS –¥–ª—è Apple Silicon")
else:
    device = torch.device("cpu")
    print("–ò—Å–ø–æ–ª—å–∑—É—é CPU")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
# model_name = "parakeet-tdt-0.6b-v2.nemo"
model_name = "stt_ru_conformer_ctc_large.nemo"
try:
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
    asr_model = nemo_asr.models.ASRModel.restore_from(model_name, map_location=device)
    # asr_model = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(model_name).cpu()   
    # asr_model.eval()
    # logger.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device}")
    # asr_model = EncDecHybridRNNTCTCModel.restore_from(model_name, map_location=device)
    print("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
except Exception as e:
    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({"status": "ok"}), 200

@app.route("/restart", methods=['GET'])
def restart():
    subprocess.run("shutdown -r 0", shell=True, check=True)
    return "Restarting"

@app.route('/transcribe', methods=['POST'])
def transcribe():
    # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –∑–∞–ø—Ä–æ—Å–∞
    print("–ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é")
    gc.collect()
    file = request.files.get('file')
    if not file:
        print("–û—à–∏–±–∫–∞: —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∑–∞–ø—Ä–æ—Å–µ")
        return jsonify({"error": "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω"}), 400

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –Ω–∞ –¥–∏—Å–∫
    file_path = "./audio.wav"
    output_path = "./audio/prepared/sample.wav"
    try:
        file.save(file_path)
        logger.info(f"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {file_path}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        return jsonify({"error": "–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞"}), 500

    try:
        logger.info(f"–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: {file_path}")
        file_path =  auto_convert_audio(file_path, output_path)

        logger.info(f"–ù–∞—á–∞–ª–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Ñ–∞–π–ª–∞: {file_path}")
        start_time = time.time()

        # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
        with torch.no_grad():
            # segments = nemo_segment_audio(
            #     file_path,
            #     segment_duration=10.0,
            #     overlap=1.5,
            #     target_sr=16000
            # )
            logger.info(os.getcwd())
            # config_path = "config/vad_inference_postprocessing.yaml"
            config_path = "config/vad_inference_frame.yaml"
            vad_in_manifest = "/app/config/vad_in_manifest.json"
            vad_out_manifest = "/app/config/vad_out_manifest.json"

            create_manifest(
                wav_path = "config/wav_paths.txt",
                manifest_filepath = vad_in_manifest
            )
          
            frame_segment_audio(
                vad_in_manifest,
                config_path,
                output_dir = "output/vad_frame"
            )

            segTable = pd.read_csv("output/vad_frame/rttm_preds/sample.txt", delim_whitespace=True, names=["start", "duration", "label"])
            segTable = segTable[segTable["label"] == "speech"]     # keep only speech

           

            audio_queries = []
            for _, row in segTable.iterrows():
                audio_queries.append({
                    # "audio_filepath": "/app/prepared/sample.wav",  # original long file
                    "offset": float(row["start"]),
                    "duration": float(row["duration"])
                })
            ds = SegmentDataset(file_path, audio_queries)
            dl = DataLoader(ds, batch_size=1, shuffle=False, num_workers=0)

            # for start_sec, end_sec, segment in segments:
            #     print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞: {start_sec:.1f}-{end_sec:.1f} —Å–µ–∫")
                
            #     # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—ç–º–ø–ª–æ–≤
            #     samples = segment.samples
            #     print(np.array(samples).shape)
               
            #     # print(np.array(samples).shape)
            #     # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä [batch, time]
            #     audio_tensor = torch.tensor(samples).unsqueeze(0).float()

            #        # Convert to NumPy array
            #     # numpy_array = tensor_data.numpy()

            #     # Now, you can serialize the NumPy array (or a dictionary containing it)
            #     data_to_serialize = {"my_tensor_data": samples.tolist()} # Convert to list for JSON
                
            #     return jsonify(data_to_serialize)

            #     # audio_tensor = prepare_audio_tensor(samples)

                
            #     # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
            #     text = asr_model.transcribe(audio_tensor)[0]
                
            #     # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
            #     transcripts.append({
            #         "start": start_sec,
            #         "end": end_sec,
            #         "text": text
            #     })
                
            #     # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            #     del audio_tensor, samples
            #     torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
            transcripts = []
            transcription_text = ""

            for audio_tensor, meta in dl:
                # audio_tensor: shape [1, N_samples], dtype float32
                audio_np = audio_tensor.squeeze(0).numpy()
     
                hyps = asr_model.transcribe(
                    audio_np, 
                    batch_size=1,
                    return_hypotheses=False,
                    timestamps=True
                )

                for hyp in hyps:
                    abstract = []
                    total = len(hyp.timestamp["segment"]) - 1
                    for i, s in enumerate(hyp.timestamp["segment"]):
                        abstract.append({
                            "start": s["start"],
                            "duration":   s["end"],
                            "text":  s["segment"]
                        })
                        if meta["duration"].tolist()[0] < 2:
                             transcription_text += s["segment"] + " "
                        else:
                            transcription_text += capitalize_text(s["segment"], i == total)
                    transcripts.append({
                        "offset" : meta["offset"].tolist()[0],
                        "duration" : meta["duration"].tolist()[0],
                        "abstract": abstract
                    })
                    if meta["duration"].tolist()[0] >= 2:
                        transcription_text += "\n"
        # –õ–æ–≥–∏—Ä—É–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        logger.info(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {time.time()-start_time:.2f} —Å–µ–∫")

        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        with open("/app/output/output.txt", "w") as f:
            f.write(f"{transcription_text}")

        print("transcripts")
        for tr in transcripts:
            print(tr)

        with open("/app/output/output.json", "w", encoding="utf-8") as f:
            f.write(json.dumps(transcripts, ensure_ascii=False))
        print("–†–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–ø–∏—Å–∞–Ω –≤ output.json")
        # return jsonify({"transcription": str(transcription_text)})
        return jsonify(transcripts)

    except Exception as e:
        # exc_type, exc_obj, exc_tb = sys.exc_info()
        # fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        logger.exception(str(e))
        return jsonify({"error": str(e)}), 500

@app.route('/qa', methods=['POST'])
def qa():
    query = request.form.get('query')
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞
    with open("/tmp/output.txt") as f:
        text = f.read()

    # 2. –ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ + –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
    embedder = HuggingFaceEmbeddings(model_name="sbert_large_nlu_ru")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = text_splitter.split_text(text)
    vector_db = Qdrant.from_texts(chunks, embedder, location=":memory:")

    # 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ QA-—Ü–µ–ø–∏
    llm = HuggingFacePipeline.from_model_id(
        model_id="IlyaGusev/saiga2_7b_lora",
        task="text-generation",
    )
    qa = RetrievalQA.from_chain_type(
        llm=llm, 
        retriever=vector_db.as_retriever(),
        chain_type="stuff"  # –∏–ª–∏ "map_reduce" –¥–ª—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
    )

    # 4. –ó–∞–¥–∞—ë–º –≤–æ–ø—Ä–æ—Å
    print(qa.run(query))

def nemo_segment_audio(
    file_path, 
    segment_duration=30.0, 
    overlap=1.0, 
    target_sr=16000
):
    """
    –°–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ—Ñ–∞–π–ª —Å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º –≤ –º–æ–Ω–æ –∏ –ø–æ–Ω–∏–∂–µ–Ω–∏–µ–º —á–∞—Å—Ç–æ—Ç—ã
    
    :param file_path: –ø—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
    :param segment_duration: –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    :param overlap: –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    :param target_sr: —Ü–µ–ª–µ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
    :return: —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–Ω–∞—á–∞–ª–æ, –∫–æ–Ω–µ—Ü, AudioSegment)
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∞—É–¥–∏–æ —Å –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π –≤ –º–æ–Ω–æ
    signal, orig_sr = sf.read(file_path)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –º–æ–Ω–æ
    if signal.ndim > 1:
        signal = signal.squeeze()  # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    
    # –ü–æ–Ω–∏–∂–∞—é—â–∞—è –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è
    if orig_sr != target_sr:
        signal = librosa.resample(signal, orig_sr=orig_sr, target_sr=target_sr)
        sample_rate = target_sr
    else:
        sample_rate = orig_sr
    
    # –†–∞—Å—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    total_duration = len(signal) / sample_rate
    segment_samples = int(segment_duration * sample_rate)
    step_samples = int((segment_duration - overlap) * sample_rate)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    segments = []
    start_sample = 0
    
    while start_sample < len(signal):
        end_sample = start_sample + segment_samples
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
        if end_sample > len(signal):
            end_sample = len(signal)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–∞ —Å–∏–≥–Ω–∞–ª–∞
        segment_signal = signal[start_sample:end_sample]
        
        # –°–æ–∑–¥–∞–Ω–∏–µ AudioSegment
        segment = AudioSegment(
            samples=segment_signal,
            sample_rate=sample_rate,
            target_sr=target_sr,
            duration=len(segment_signal) / sample_rate,
            offset=start_sample / sample_rate,
            orig_sr=orig_sr
        )
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        segments.append((
            start_sample / sample_rate,  # –Ω–∞—á–∞–ª–æ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            end_sample / sample_rate,    # –∫–æ–Ω–µ—Ü –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            segment
        ))
        
        # –ü–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —Å–µ–≥–º–µ–Ω—Ç—É
        start_sample += step_samples
        
        # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü
        if start_sample >= len(signal):
            break
    
    return segments


    # signal, sr = librosa.load(file_path, sr=16000, mono=True)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞
    manifest = [{
        "audio_filepath": file_path,
        "duration": librosa.get_duration(file_path),
        "text": ""  # –¢–µ–∫—Å—Ç –Ω–µ –Ω—É–∂–µ–Ω –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    }]

    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    segmented_manifest = create_segments(
        manifest=manifest,
        segment_duration=60.0,
        step_duration=58.5,  # 60 - 1.5 overlap
        min_segment_duration=1.0,
        output_dir="temp_segments",
    )

    for entry in segmented_manifest:
        segment_path = entry[file_path]
        transcription = asr_model.transcribe([segment_path])[0]
        transcripts.append(transcription)

        
        for i, seg in enumerate(segments):
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –±–∞—Ç—á–∞
            seg_tensor = torch.tensor(seg).unsqueeze(0)
            transcripts = []

            # transcription = asr_model.transcribe(seg_tensor, channel_selector=0)
            if seg_tensor.dim() == 3:
                print("–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å—Ç–µ—Ä–µ–æ –≤ –º–æ–Ω–æ...")
                seg_tensor = seg_tensor.mean(dim=2) if seg_tensor.size(2) > 1 else seg_tensor[:, :, 0]
            
            text = model.transcribe(seg_tensor)[0]
            transcripts.append(text)
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            # del seg_tensor
            # torch.cuda.empty_cache() if torch.cuda.is_available() else Non
        return " ".join(transcripts)

def auto_convert_audio(file_path, output_path):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –ª—é–±–æ–π —Ñ–æ—Ä–º–∞—Ç –≤ WAV 16kHz mono"""

    # name = Path(file_path).name
    try:
        subprocess.run([
            "ffmpeg", "-i", file_path,
            "-ac", "1",          # –º–æ–Ω–æ
            "-ar", "16000",      # 16kHz
            "-acodec", "pcm_s16le",
            "-f", "wav",
            "-loglevel", "error",
            "-vn",
            "-y",                # –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            output_path,
        ], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    except subprocess.CalledProcessError as e:
        error_msg = e.stderr.decode() if e.stderr else "Unknown FFmpeg error"
        raise RuntimeError(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {error_msg}") from e
    
    return output_path

def combine_segments_with_overlap(results, min_overlap_words=3, max_overlap_words=5, similarity_threshold=0.8):
    """
    –°–∫–ª–µ–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
    
    :param results: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π [{"start": float, "end": float, "text": str}, ...]
    :param min_overlap_words: –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–∏
    :param max_overlap_words: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–∏
    :param similarity_threshold: –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ñ—Ä–∞–∑
    :return: —Å–∫–ª–µ–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if not results:
        return ""
    
    full_text = results[0]['text']
    prev_text = full_text
    
    for i in range(1, len(results)):
        current = results[i]
        current_text = current['text']
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –Ω–∞ —Å–ª–æ–≤–∞
        prev_words = prev_text.split()
        curr_words = current_text.split()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –ø–æ–∏—Å–∫–∞ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        search_range = range(min_overlap_words, min(max_overlap_words, len(prev_words), len(curr_words)) + 1)
        
        # –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        best_match = None
        best_similarity = 0
        
        for n in search_range:
            if n > len(prev_words) or n > len(curr_words):
                continue
                
            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ n —Å–ª–æ–≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
            prev_overlap = " ".join(prev_words[-n:])
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ n —Å–ª–æ–≤ —Ç–µ–∫—É—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
            curr_overlap = " ".join(curr_words[:n])
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
            similarity = difflib.SequenceMatcher(
                None, prev_overlap.lower(), curr_overlap.lower()
            ).ratio()
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = (n, prev_overlap, curr_overlap)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        if best_match and best_similarity >= similarity_threshold:
            n, prev_phrase, curr_phrase = best_match
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â—É—é—Å—è —á–∞—Å—Ç—å
            remaining_text = " ".join(curr_words[n:])
            
            # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ –º–æ–∂–Ω–æ –≤—ã–≤–æ–¥–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∫–ª–µ–π–∫–µ
            print(f"üîó –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ {n} —Å–ª–æ–≤: "
                  f"¬´{prev_phrase}¬ª ‚Üí ¬´{curr_phrase}¬ª "
                  f"(—Å—Ö–æ–¥—Å—Ç–≤–æ: {best_similarity:.2f})")
            
            full_text += " " + remaining_text
        else:
            # –ï—Å–ª–∏ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –¥–æ–±–∞–≤–ª—è–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
            full_text += " " + current_text
            if best_match:
                print(f"‚ö†Ô∏è –ù–∏–∑–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ ({best_similarity:.2f}): "
                      f"¬´{best_match[1]}¬ª vs ¬´{best_match[2]}¬ª")
        
        prev_text = current_text
    
    return full_text.strip()

def prepare_audio_tensor(audio_data):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
    :param audio_data: –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ (numpy array –∏–ª–∏ tensor)
    :return: —Ç–µ–Ω–∑–æ—Ä –≤ —Ñ–æ—Ä–º–∞—Ç–µ [batch, time]
    """
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä PyTorch, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
    if not isinstance(audio_data, torch.Tensor):
        audio_tensor = torch.tensor(audio_data)
    else:
        audio_tensor = audio_data
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    if audio_tensor.dim() == 1:  # [time]
        audio_tensor = audio_tensor.unsqueeze(0)  # [1, time]
    elif audio_tensor.dim() == 3:  # [batch, channels, time]
        # –£–±–∏—Ä–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–æ–≤
        audio_tensor = audio_tensor.squeeze(1)  # [batch, time]
    elif audio_tensor.dim() == 2 and audio_tensor.size(0) != 1:
        # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å [time, channels], —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º
        audio_tensor = audio_tensor.transpose(0, 1)
        audio_tensor = audio_tensor.squeeze(1)  # [batch, time]
    
    return audio_tensor.float()

def capitalize_text(text, last_sent = False):
    if not text.strip():  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –∏–ª–∏ –ø—Ä–æ–±–µ–ª—ã
        return ". "
    cleaned = text.rstrip()  # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –∫–æ–Ω—Ü–µ

    end_space =  " " if not last_sent else ""
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–æ—á–∫–∏, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç, –∏ –ø—Ä–æ–±–µ–ª–∞
    result = cleaned + '.' * (not cleaned.endswith('.')) + end_space
    # –ü–æ–∏—Å–∫ –ø–µ—Ä–≤–æ–π –±—É–∫–≤—ã –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –∑–∞–≥–ª–∞–≤–Ω—É—é
    for i, char in enumerate(result):
        if char.isalpha():
            return result[:i] + char.upper() + result[i+1:]
    return result  # –í–æ–∑–≤—Ä–∞—Ç –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –µ—Å–ª–∏ –±—É–∫–≤ –Ω–µ—Ç

if __name__ == '__main__':
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–µ—Ä Flask —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏
    print("–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ Flask –Ω–∞ 0.0.0.0:5000")
    app.run(host='0.0.0.0', port=5000, threaded=True)
